{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f908bb92-0a5c-4eef-9bae-c15eeda234c8",
   "metadata": {},
   "source": [
    "# Transfomer model\n",
    "https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78241a41-a23f-48d1-b4ea-238ac5f9c0f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T00:26:46.055478Z",
     "start_time": "2024-02-14T00:26:46.041375Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d2cf5a-aeb5-48f2-b01a-9580ba7718c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -1.8421, -3.6841, -5.5262, -7.3683])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 10\n",
    "\n",
    "torch.arange(0, 10, 2).float() * (-math.log(10000.0) / d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa668999-cf00-4532-a7bc-f614f5668e6a",
   "metadata": {},
   "source": [
    "## Input Embedding\n",
    "\n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
    "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
    "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
    "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √\n",
    "dmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02eee918-0e0b-4362-b7ba-d75d0e8f57be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T00:24:26.889389Z",
     "start_time": "2024-02-14T00:24:26.862849Z"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d43de-ecde-4365-90ea-8cff5178d9a9",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "learned and fixed [9].\n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:\n",
    "\n",
    "P E(pos,2i) = sin(pos/100002i/dmodel)\n",
    "P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
    "\n",
    "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\n",
    "P Epos.\n",
    "\n",
    "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
    "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
    "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
    "during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9a2644-a7dc-4cdf-87d9-1326ad70fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # Apply sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951991e-4545-4f57-aee4-a7f0b63e7f4a",
   "metadata": {},
   "source": [
    "## Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a510dd-5eef-4a13-8f9b-06cdde78225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # Multiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # Added\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55da399-e97d-40c6-8fdc-b52334edced5",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc7189e-cde6-423c-8ed8-02f7020166a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e6abb-ae76-430d-ae04-c5a30c1f3b6f",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0e973e-fdc7-46e0-99ca-5cc4a1cd8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        self.d_k = d_mode // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contigous(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9a782-09df-49a2-b6fc-1472f8a19d03",
   "metadata": {},
   "source": [
    "## Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009c1c5f-2949-4c66-893b-4216aba7592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a95c31-d4dd-4e7e-83ca-0714a147306c",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691142dd-b754-42e2-8d5b-5b7b3f987b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        self_attention_block: MultiHeadAttentionBlock,\n",
    "        feed_forward_block: FeedForwardBlock,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList(\n",
    "            [ResidualConnection(dropout) for _ in range(2)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask, dropout)\n",
    "        )\n",
    "        x = self.residual_connections[1](x, lambda x: self.feed_forward_block(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e21801-748d-4ebc-a9be-36892064b837",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51303e72-a5f4-4f2c-8740-0ce069c9bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a44418-6d50-4e9e-89cf-166247bd77ed",
   "metadata": {},
   "source": [
    "## Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a2d99d-a7ca-46ed-8e55-da237c6fbd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        self_attention_block: MultiHeadAttentionBlock,\n",
    "        cross_attention_block: MultiHeadAttentionBlock,\n",
    "        feed_forward_block: FeedForwardBlock,\n",
    "        dropout: float\n",
    "    ):\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList(\n",
    "            [ResidualConnection(dropout) for _ in range(3)]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, lambda x: self.feed_forward_block(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5972dd-c006-4079-9cf6-e443711d0593",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6b2f0a3-441d-481f-8ed8-2d8c12351e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c29f5-a7be-41ad-acde-a7e481519d39",
   "metadata": {},
   "source": [
    "## Projection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bcabb-0b90-4cf6-90f2-5d9d5405d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
